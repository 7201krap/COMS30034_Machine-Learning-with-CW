{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Principal Component Analysis (PCA) and Independent Component Analysis (ICA)\n",
    "\n",
    "The aim of this lab is to understand and perform Principal component analysis (PCA) for a given dataset. \n",
    "PCA is a technique that is widely used for applications such as dimensionality reduction, lossy data compression, feature extraction and data visualisation.\n",
    "\n",
    "Let's start by importing the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import KernelPCA, PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) PCA implementation\n",
    "\n",
    "PCA consists of two main steps:\n",
    "1. Find a new set of basis vectors, named the principal components, according to how much of the variance within the dataset each of these components are able to capture. The first principal component corresponds to the one which captures the most variance, the second principal component corresponds to the second most variance and so on.\n",
    "2. Select the first $n$ principal components and project the original data to this new basis.\n",
    "\n",
    "In particular, the principal components correspond to the eigenvectors of the covariance matrix of the data. The corresponding eigenvalues capture the amount of variance in that direction.\n",
    "\n",
    "The new space is given by projecting the data onto the eigenvectors of the covariance matrix:\n",
    "$$ S = XU$$\n",
    "where $X$ is the $N \\times D$ matrix containing the original data, $S$ is an $N \\times D$ matrix representing the projected data (also called scores), and $U$ is an $D \\times D$ orthonormal matrix where each of the columns represent an eigenvector of the covariance matrix.\n",
    "\n",
    "In this section you will start by computing the eigenvalues and eigenvectors of a covariance matrix. You will the project the data onto these eigenvectors to obtain points in the new space and visualise the result. \n",
    "\n",
    "### 1.1) Eigenvectors and eigenvalues \n",
    "Computing eigenvectors is critical for performing PCA. Given a dataset `X` and a covariance matrix ```cov_matrix```, your task is to:\n",
    "1. Find the eigenvalues and eigenvector of the covariance matrix. (hint: ```np.linalg.eigh()```)\n",
    "2. Sort the eigenvalues in descending order. (use the provided function ```sort_evals_descending()```)\n",
    "3. Plot both the data and the new eigenvectors. (use the provided function ```plot_basis_vectors()```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0, 0])\n",
    "orig_cov_matrix = [[1.0, 0.8], [0.8, 1.0]]\n",
    "samples = 1000\n",
    "X = np.random.multivariate_normal(mean, orig_cov_matrix, size=samples)\n",
    "indices_for_sorting = np.argsort(X[:, 0])\n",
    "X = X[indices_for_sorting, :]\n",
    "cov_matrix = [[0.96595536, 0.79103839],[0.79103839, 1.00568202]]\n",
    "\n",
    "def sort_evals_descending(evals, evectors):\n",
    "    \"\"\"\n",
    "    Sorts eigenvalues and eigenvectors in decreasing order. This function\n",
    "    also aligns the first two eigenvectors to be in first two quadrants if\n",
    "    the data is 2D (remember that any eigenvector's direction can be inverted\n",
    "    and it is still an eigenvector with the same eigenvalue). \n",
    "    \"\"\"\n",
    "\n",
    "    index = np.flip(np.argsort(evals))\n",
    "    evals = evals[index]\n",
    "    evectors = evectors[:, index]\n",
    "    if evals.shape[0] == 2:\n",
    "        if np.arccos(np.matmul(evectors[:, 0], 1 / np.sqrt(2) * np.array([1, 1]))) > np.pi / 2:\n",
    "            evectors[:, 0] = -evectors[:, 0]\n",
    "        if np.arccos(np.matmul(evectors[:, 1], 1 / np.sqrt(2) * np.array([-1, 1]))) > np.pi / 2:\n",
    "            evectors[:, 1] = -evectors[:, 1]\n",
    "    return evals, evectors\n",
    "\n",
    "def plot_basis_vectors(X, evectors):\n",
    "    \"\"\"\n",
    "    Plots bivariate data as well as new basis vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=[4, 4])\n",
    "    plt.plot(X[:, 0], X[:, 1], '.', color=[.5, .5, .5], label='Data')\n",
    "    plt.axis('equal')\n",
    "    plt.xlabel('X_1')\n",
    "    plt.ylabel('X_2')\n",
    "    plt.plot([0, evectors[0, 0]], [0, evectors[1, 0]], color='r', linewidth=3,\n",
    "           label='Basis vector 1')\n",
    "    plt.plot([0, evectors[0, 1]], [0, evectors[1, 1]], color='b', linewidth=3,\n",
    "           label='Basis vector 2')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CORRECT ANSWER##\n",
    "\n",
    "# Calculate the eigenvalues and eigenvectors\n",
    "evals, evectors = np.linalg.eigh(cov_matrix)\n",
    "# Sort the eigenvalues in descending order\n",
    "evals, evectors = sort_evals_descending(evals, evectors)\n",
    "\n",
    "plot_basis_vectors(X, evectors)\n",
    "\n",
    "##CORRECT ANSWER##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Projecting the data onto the eigenvectors\n",
    "Next, create a function ```pca(X)``` to perform PCA with given data input `X`. The main steps you need to implement are:\n",
    "1. Obtain the covariance matrix $S$ from `X` (where $\\bar{x}$ is the mean):\n",
    "$$ S = \\frac{1}{N} \\sum_{n=1}^N (x_n- \\bar{x})(x_n- \\bar{x})^T $$\n",
    "2. Find the eigenvalues, eigenvectors and sort them (as you did in the previous exercise)\n",
    "3. Project the initial data, `X`, onto the newly obtained eigenvectors  (hint: ```np.matmul()```)\n",
    "4. The function should return 3 parameters:\n",
    "    * `score`: the data projected onto the new basis\n",
    "    * `evectors`: eigenvectors\n",
    "    * `evals`: corresponding eigenvalues\n",
    "    \n",
    "Finally plot the score by utilising the provided function ```plot_data_new_basis(Y)``` where the input Y is the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CORRECT ANSWER##\n",
    "\n",
    "def pca(X):\n",
    "    \"\"\"\n",
    "    Performs PCA on multivariate data.\n",
    "\n",
    "    Args:\n",
    "    X (numpy array of floats) : Data matrix each column corresponds to a\n",
    "                                different random variable\n",
    "\n",
    "    Returns:\n",
    "    (numpy array of floats)   : Data projected onto the new basis\n",
    "    (numpy array of floats)   : eigenvectors\n",
    "    (numpy array of floats)   : corresponding eigenvalues\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Subtract the mean of X\n",
    "    X_bar = X - np.mean(X, axis=0)\n",
    "    # Calculate the sample covariance matrix\n",
    "    cov_matrix = 1 / X.shape[0] * np.matmul(X_bar.T, X_bar)\n",
    "    # Calculate the eigenvalues and eigenvectors\n",
    "    evals, evectors = np.linalg.eigh(cov_matrix)\n",
    "    # Sort the eigenvalues in descending order\n",
    "    evals, evectors = sort_evals_descending(evals, evectors)\n",
    "    # Project the data onto the new eigenvector basis\n",
    "    score = np.matmul(X, evectors)\n",
    "\n",
    "    return score, evectors, evals\n",
    "\n",
    "##CORRECT ANSWER##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_new_basis(Y):\n",
    "    \"\"\"\n",
    "    Plots bivariate data after transformation to new bases. Similar to plot_data\n",
    "    but with colors corresponding to projections onto basis 1 (red) and\n",
    "    basis 2 (blue).\n",
    "    The title indicates the sample correlation calculated from the data.\n",
    "\n",
    "    Note that samples are re-sorted in ascending order for the first random\n",
    "    variable.\n",
    "\n",
    "    Args:\n",
    "    Y (numpy array of floats) : Data matrix in new basis each column\n",
    "                                corresponds to a different random variable\n",
    "\n",
    "    Returns:\n",
    "    Nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize=[8, 4])\n",
    "    gs = fig.add_gridspec(2, 2)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.plot(Y[:, 0], 'r')\n",
    "    plt.ylabel('Projection \\n basis vector 1')\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    ax2.plot(Y[:, 1], 'b')\n",
    "    plt.xlabel('Sample number')\n",
    "    plt.ylabel('Projection \\n basis vector 2')\n",
    "    ax3 = fig.add_subplot(gs[:, 1])\n",
    "    ax3.plot(Y[:, 0], Y[:, 1], '.', color=[.5, .5, .5])\n",
    "    ax3.axis('equal')\n",
    "    plt.xlabel('Projection basis vector 1')\n",
    "    plt.ylabel('Projection basis vector 2')\n",
    "    plt.title('Sample corr: {:.1f}'.format(np.corrcoef(Y[:, 0], Y[:, 1])[0, 1]))\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# Perform PCA on the data matrix X\n",
    "score, evectors, evals = pca(X)\n",
    "# Plot the data projected into the new basis\n",
    "plot_data_new_basis(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Dimensionality reduction with PCA\n",
    "A key application of PCA is dimensionality reduction, which will be the focus of the next task.\n",
    "\n",
    "Previously we looked at a toy dataset of simple 2D data. We will now introduce the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset and perform PCA on it. This dataset contains a large number of handwritten digits and is widely used to benchmark many machine learning algorithms. \n",
    "\n",
    "Run the following cell to load the MNIST dataset from [scikit-learn datasets](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html#sklearn.datasets.fetch_openml) (this may take a little while) and inspect its sample output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_MNIST_sample(X):\n",
    "    \"\"\"\n",
    "    Plots 9 images in the MNIST dataset.\n",
    "\n",
    "    Args:\n",
    "     X (numpy array of floats) : Data matrix each column corresponds to a\n",
    "                                 different random variable\n",
    "\n",
    "    Returns:\n",
    "    Nothing.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    k = 0\n",
    "    for k1 in range(3):\n",
    "        for k2 in range(3):\n",
    "            k = k + 1\n",
    "            plt.imshow(np.reshape(X[k, :], (28, 28)),\n",
    "                     extent=[(k1 + 1) * 28, k1 * 28, (k2+1) * 28, k2 * 28],\n",
    "                     vmin=0, vmax=255)\n",
    "    plt.xlim((3 * 28, 0))\n",
    "    plt.ylim((3 * 28, 0))\n",
    "    plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
    "                  labelbottom=False)\n",
    "    plt.clim([0, 250])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "mnist = fetch_openml(name='mnist_784')\n",
    "X = mnist.data\n",
    "plot_MNIST_sample(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Scree plot\n",
    "Using the `pca(X)` function that you implemented in the exercise 1.2, calculate the eigenvalues and eigenvectors of the MNIST data `X`. Plot the eigenvalues according to their values (utilising the provided `plot_eigenvalues()` function), a graphical representation known as a scree plot.\n",
    "\n",
    "What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eigenvalues(evals, limit=True):\n",
    "    \"\"\"\n",
    "    Plots eigenvalues.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(np.arange(1, len(evals) + 1), evals, 'o-k')\n",
    "    plt.xlabel('Component')\n",
    "    plt.ylabel('Eigenvalue')\n",
    "    plt.title('Scree plot')\n",
    "    if limit:\n",
    "        plt.xlim([0, 100])  # limit x-axis up to 100 for zooming\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CORRECT ANSWER##\n",
    "\n",
    "# perform PCA\n",
    "score, evectors, evals = pca(X)\n",
    "\n",
    "# plot the eigenvalues\n",
    "plot_eigenvalues(evals, limit=True)\n",
    "\n",
    "##CORRECT ANSWER##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Reconstructed data\n",
    "Next, select just the first `K` components (discarding the remaining `D-K` components) and use the scores to project the data back into the orignal space of `X`.\n",
    "\n",
    "Implement a `reconstruct_data(score, evectors, X_mean, K)` function which reconstructs the data on the first `K` components from the score and eigenvectors (hint: look at step 3. of exercise 1.2 and don't forget to add the mean). The function should return the reconstructed data, `X_reconstructed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CORRECT ANSWER##\n",
    "\n",
    "def reconstruct_data(score, evectors, X_mean, K):\n",
    "    \"\"\"\n",
    "    Reconstruct the data based on the first K components.\n",
    "    \"\"\"\n",
    "    assert(0<K<=784),\"K must be between 1 and 784\"\n",
    "\n",
    "    X_reconstructed =  np.matmul(score[:, :K], evectors[:, :K].T) + X_mean\n",
    "    \n",
    "    return X_reconstructed\n",
    "\n",
    "##CORRECT ANSWER##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below which plots the original image and the reconstructed one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 100 # 0<K<=784 (total number of pixel values)\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_reconstructed = reconstruct_data(score, evectors, X_mean, K)\n",
    "\n",
    "plot_MNIST_sample(X)\n",
    "plot_MNIST_sample(X_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with changing the number of components, `K`. How small can you make `K` without losing too much quality? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Kernel PCA\n",
    "\n",
    "Instead of performing PCA on our initial data, PCA can be performed on a transformed feature space. For example, a nonlinear transformation $\\phi(x)$ in an $M$-dimensional feature space will project each data point $x_n$ onto a point $\\phi(x_n)$. Standard PCA can be performed in the new feature space which implicitly defines a non-linear principal component in the original data space.\n",
    "\n",
    "The example below (from Bishop chapter 12.3) should help to illustrate this idea:\n",
    "\n",
    "<img src=\"kernelPCA.png\" width=\"400\">\n",
    "\n",
    "Reporting the excellent answer from (https://stats.stackexchange.com/questions/94463/what-are-the-advantages-of-kernel-pca-over-standard-pca): \"The data points here (on the left) are located mostly along a curve in 2D. PCA cannot reduce the dimensionality from two to one, because the points are not located along a straight line. But still, the data are \"obviously\" located around a one-dimensional non-linear curve. ... kernel PCA can find this non-linear manifold and discover that the data are in fact nearly one-dimensional\".\n",
    "\n",
    "Your next task is to perform kernel PCA on the moon and circles dataset available from ```sklearn.datasets```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {'moons': make_moons(n_samples=100, noise=0.075, random_state=300),\n",
    "            'circles': make_circles(n_samples=150, noise=0.1, factor=0.2, random_state=300)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the two datasets above, let's present two scatter plots and visualise each dataset distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moons, y_moons = datasets['moons']\n",
    "X_circles, y_circles = datasets['circles']\n",
    "figure = plt.figure(figsize=(12, 8))\n",
    "plt.subplot(221)\n",
    "plt.scatter(X_moons[:,0], X_moons[:, 1], c=y_moons)\n",
    "plt.subplot(222)\n",
    "plt.scatter(X_circles[:,0], X_circles[:, 1], c=y_circles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Perform Kernel PCA\n",
    "Write a function `kernel_PCA(dataset,gamma)` to perform KernelPCA (hint: check [KernelPCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html)). Your function should take the dataset and a `gamma` value as input and return the transformed input data `X_transformed`. You should set the `n_components` parameter of `KernelPCA()` to 2 and `kernel='rbf'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CORRECT ANSWER##\n",
    "def kernel_PCA(dataset, gamma):\n",
    "    kpca = KernelPCA(n_components=2, kernel='rbf', gamma=gamma)\n",
    "    \n",
    "    X, y = dataset\n",
    "    X_transformed = kpca.fit_transform(X)\n",
    "    return X_transformed\n",
    "##CORRECT ANSWER##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moons_transformed = kernel_PCA(datasets['moons'], gamma=15)\n",
    "X_circles_transformed = kernel_PCA(datasets['circles'], gamma=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Plot transformed data\n",
    "Next plot the trasformed data with the corresponding labels (hint: look at how we plotted the original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CORRECT ANSWER##\n",
    "figure = plt.figure(figsize=(12, 8))\n",
    "plt.subplot(221)\n",
    "plt.scatter(X_moons_transformed[:,0], X_moons_transformed[:, 1], c=y_moons)\n",
    "plt.subplot(222)\n",
    "plt.scatter(X_circles_transformed[:,0], X_circles_transformed[:, 1], c=y_circles)\n",
    "##CORRECT ANSWER##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the original data, how did the feature space change? Can you think of any tasks that could benefit from performing Kernel PCA?\n",
    "\n",
    "As you can see for the moon dataset you cannot separate the two labels completely, go back to the cell where the dataset is created and change the noise value to 0. Re-run all the cells again and compare the plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) ICA\n",
    "\n",
    "Independent component analysis (ICA) is a method that allows us to separate individual components from a mixed signal. This problem is also named \"blind source separation\" because we are only given the mixed data and do not have access to the original sources or their mixing coefficients. Similarly to PCA, the goal of ICA is to find a linear transformation from a set of latent variables to our data. The key difference is that PCA assumes that all latents are Gaussian distributed and ICA requires that each latent is non-Gaussian (see the [lecture notes on  ICA](https://uob-coms30035.github.io/JamesLectures/ica.pdf)).\n",
    "\n",
    "For our next exercise, we will look at three images where the content is mixed from three individual sources. The aim is to separate each source through [scikit-learn's FastICA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html?highlight=fastica#sklearn.decomposition.FastICA) and compare to the traditional PCA algorithm.\n",
    "\n",
    "Run the next cell to load the data and visualise the mixed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1 = np.genfromtxt('mixed_img_1.dat', dtype='float64')\n",
    "img_2 = np.genfromtxt('mixed_img_2.dat', dtype='float64')\n",
    "img_3 = np.genfromtxt('mixed_img_3.dat', dtype='float64')\n",
    "\n",
    "f, (ax1,ax2,ax3) = plt.subplots(1,3)\n",
    "f.set_size_inches((24,40))\n",
    "ax1.imshow(img_1, cmap=plt.cm.gray)\n",
    "ax1.set_title(\"Mixed image 1\")\n",
    "ax2.imshow(img_2, cmap=plt.cm.gray)\n",
    "ax2.set_title(\"Mixed image 2\")\n",
    "ax3.imshow(img_3, cmap=plt.cm.gray)\n",
    "ax3.set_title(\"Mixed image 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) Prepare the data\n",
    "First, flatten each image and stack them together (hint: ```image.flatten()``` and ```np.vstack()```).\n",
    "Check that the shape of the new data is `(3, 262144)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CORRECT ANSWER##\n",
    "# each image is flattened and all three are stacked together\n",
    "X = np.vstack((img_1.flatten(),img_2.flatten(),img_3.flatten()))\n",
    "X.shape\n",
    "##CORRECT ANSWER##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2) Perform PCA\n",
    "Initialise a PCA class with 3 components (using [scikit-learn's PCA implementation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)) and train with the given input data obtained from the previous cell (hint: ```fit_transform()```, remember to transpose the input data ```X``` if the size is ```(3, 262144)```).\n",
    "Show the images corresponding to each principal component (hint: look at the cell where the all the original mixed images are shown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CORRECT ANSWER##\n",
    "pca = PCA(n_components=3)\n",
    "img_pca= pca.fit_transform(X.T)\n",
    "\n",
    "f, (ax1,ax2,ax3) = plt.subplots(1,3)\n",
    "f.set_size_inches((24,40))\n",
    "ax1.imshow(img_pca[:,0].reshape((512,512)), cmap=plt.cm.gray)\n",
    "ax1.set_title(\"Principal component 1\")\n",
    "ax2.imshow(img_pca[:,1].reshape((512,512)), cmap=plt.cm.gray)\n",
    "ax2.set_title(\"Principal component 2\")\n",
    "ax3.imshow(img_pca[:,2].reshape((512,512)), cmap=plt.cm.gray)\n",
    "ax3.set_title(\"Principal component 3\")\n",
    "##CORRECT ANSWER##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3) Perform Fast ICA\n",
    "Repeat the same process of the previous cell using [FastICA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html) with 3 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CORRECT ANSWER##\n",
    "ica = FastICA(n_components=3)\n",
    "img_ica= ica.fit_transform(X.T)\n",
    "\n",
    "f, (ax1,ax2,ax3) = plt.subplots(1,3)\n",
    "f.set_size_inches((24,40))\n",
    "ax1.imshow(img_ica[:,0].reshape((512,512)), cmap=plt.cm.gray)\n",
    "ax1.set_title(\"Independent component 1\")\n",
    "ax2.imshow(img_ica[:,1].reshape((512,512)), cmap=plt.cm.gray)\n",
    "ax2.set_title(\"Independent component 2\")\n",
    "ax3.imshow(img_ica[:,2].reshape((512,512)), cmap=plt.cm.gray)\n",
    "ax3.set_title(\"Independent component 3\")\n",
    "##CORRECT ANSWER##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What considerations can you make while comparing FastICA to PCA? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "Congratulations, you have reached the end of this lab. Let's summarise what we learnt.\n",
    "* Understand the role of eigenvectors and eigenvalues in relation to the PCA.\n",
    "* Main steps behind the PCA algorithm.\n",
    "* One application of the PCA with dimensionality reduction with the handwritten digits from the MNIST dataset.\n",
    "* Understand the importance of Kernel PCA and FastICA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "* COMS30035 Machine Learning\n",
    "* Bishop - Pattern Recognition and Machine Learning: Chapter 12 - Continuous Latent Variables \n",
    "* [Transforming nonlinear data with Kernel PCA](https://www.kaggle.com/lambdaofgod/kernel-pca-examples)\n",
    "* Pablo de Castro: [SepFuentes](https://github.com/pablodecm/SepFuentes)\n",
    "* [Neuromatch Academy 2020](https://www.neuromatchacademy.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End :)\n",
    "\n",
    "<video controls src=\"gif.mp4\" width=\"200\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
